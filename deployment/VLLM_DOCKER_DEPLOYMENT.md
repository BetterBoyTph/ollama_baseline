# ç”„å¬›æ¨¡å‹ Docker vLLM éƒ¨ç½²æŒ‡å—

åŸºäº Qwen2.5-0.5B + LoRA å¾®è°ƒçš„ç”„å¬›è§’è‰²æ¨¡å‹ Docker vLLM éƒ¨ç½²æŒ‡å—

## ğŸ“‹ å‰ææ¡ä»¶

### 1. ç¡®è®¤è®­ç»ƒå®Œæˆ
ç¡®ä¿ä»¥ä¸‹æ–‡ä»¶å­˜åœ¨ï¼š
```
training/training/models/huanhuan_fast/
â”œâ”€â”€ adapter_config.json
â”œâ”€â”€ adapter_model.safetensors
â””â”€â”€ train_results.json
```

### 2. å®‰è£… Docker å’Œ NVIDIA Container Toolkit
```bash
# Ubuntuå®‰è£…Docker
sudo apt update
sudo apt install docker.io

# å®‰è£…NVIDIA Container Toolkit
distribution=$(. /etc/os-release;echo $ID$VERSION_ID)
curl -s -L https://nvidia.github.io/libnvidia-container/gpgkey | sudo apt-key add -
curl -s -L https://nvidia.github.io/libnvidia-container/$distribution/libnvidia-container.list | sudo tee /etc/apt/sources.list.d/nvidia-container-toolkit.list

sudo apt update
sudo apt install -y nvidia-container-toolkit

# é‡å¯DockeræœåŠ¡
sudo systemctl restart docker
```

### 3. éªŒè¯ç¯å¢ƒ
```bash
# éªŒè¯Dockerå®‰è£…
docker --version

# éªŒè¯NVIDIA Dockeræ”¯æŒ
docker run --rm --gpus all nvidia/cuda:11.0-base-ubuntu20.04 nvidia-smi
```

## ğŸ³ Docker vLLM éƒ¨ç½²æ–¹å¼

### 1. å‡†å¤‡æ¨¡å‹æ–‡ä»¶
å°†è®­ç»ƒå¥½çš„LoRAæ¨¡å‹æ–‡ä»¶è½¬æ¢ä¸ºvLLMå…¼å®¹æ ¼å¼ï¼š
```bash
# ç¡®ä¿æ¨¡å‹æ–‡ä»¶åœ¨æŒ‡å®šä½ç½®
ls training/training/models/huanhuan_fast/

# å¦‚æœè¿˜æ²¡æœ‰è½¬æ¢æ¨¡å‹ï¼Œæ‰§è¡Œè½¬æ¢
python convert_lora_to_gguf.py
```

### 2. æ‹‰å–vLLM Dockeré•œåƒ
```bash
# æ‹‰å–vLLMå®˜æ–¹é•œåƒ
docker pull vllm/vllm-openai:latest
```

### 3. å¯åŠ¨Dockerå®¹å™¨
```bash
# è¿è¡ŒvLLMå®¹å™¨ (é€‚ç”¨äºRTX 3090 24Gç¯å¢ƒ)
docker run --gpus all \
    --name huanhuan-vllm \
    -v $(pwd)/training/training/models/huanhuan_fast:/models/huanhuan_fast \
    -v $(pwd)/deployment:/deployment \
    -p 8000:8000 \
    --env NCCL_IGNORE_DISABLED_P2P=1 \
    vllm/vllm-openai:latest \
    --model Qwen/Qwen2.5-0.5B-Instruct \
    --adapter /models/huanhuan_fast \
    --host 0.0.0.0 \
    --port 8000 \
    --enable-lora \
    --max-lora-rank 64 \
    --tensor-parallel-size 1 \
    --gpu-memory-utilization 0.8 \
    --max-model-len 4096 \
    --enforce-eager
```

### 4. éªŒè¯éƒ¨ç½²
```bash
# æµ‹è¯•API
curl -X POST http://localhost:8000/v1/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "Qwen/Qwen2.5-0.5B-Instruct",
    "prompt": "ä½ æ˜¯è°ï¼Ÿ",
    "max_tokens": 200
  }'
```

## âš¡ å¿«é€Ÿéƒ¨ç½²æ–¹å¼ (é€‚ç”¨äºAutodlå®ä¾‹)

### 1. å®‰è£…ä¾èµ–
```bash
# æ¿€æ´»è™šæ‹Ÿç¯å¢ƒ
source /root/autodl-tmp/huanhuan_env/bin/activate

# å®‰è£…vLLM
pip install vllm==0.5.4

# éªŒè¯å®‰è£…
python -c "import vllm; print(vllm.__version__)"
```

### 2. å¯åŠ¨vLLMæœåŠ¡
```bash
# ç¡®ä¿åœ¨é¡¹ç›®æ ¹ç›®å½•
cd /path/to/ollama_baseline

# å¯åŠ¨vLLMæœåŠ¡ (é’ˆå¯¹RTX 3090 24Gé…ç½®ä¼˜åŒ–)
python -m vllm.entrypoints.openai.api_server \
    --model Qwen/Qwen2.5-0.5B-Instruct \
    --adapter ./training/training/models/huanhuan_fast \
    --host 0.0.0.0 \
    --port 8000 \
    --enable-lora \
    --max-lora-rank 64 \
    --tensor-parallel-size 1 \
    --gpu-memory-utilization 0.8 \
    --max-model-len 4096 \
    --enforce-eager
```

## âš™ï¸ ç³»ç»Ÿé…ç½®å’Œå¹¶å‘å‚æ•°è¯´æ˜

### å¹¶å‘å‚æ•°è®¾ç½®è¯´æ˜

æ ¹æ®ç³»ç»Ÿé…ç½® (RTX 3090 24G, 16æ ¸å¿ƒCPU, 120Gå†…å­˜) åŠ¨æ€è®¾ç½®äº†ä»¥ä¸‹å‚æ•°ï¼š

1. `MAX_CONCURRENT_REQUESTS` åŠ¨æ€è®¡ç®—:
   - æ ¹æ®CPUç‰©ç†æ ¸å¿ƒæ•°çš„ä¸€åŠè®¡ç®—
   - æœ€å°å€¼ä¸º1ï¼Œæœ€å¤§å€¼ä¸º8
   - å¯¹äº16æ ¸å¿ƒCPUï¼Œè®¾ç½®ä¸º8

2. `--tensor-parallel-size 1`:
   - å•GPUè®¾ç½®ï¼Œæ— éœ€å¼ é‡å¹¶è¡Œ

3. `--gpu-memory-utilization 0.8`:
   - é™åˆ¶GPUå†…å­˜ä½¿ç”¨ç‡ï¼Œä¿ç•™ç©ºé—´ç»™ç³»ç»Ÿå’Œå…¶ä»–è¿›ç¨‹

4. `--max-model-len 4096`:
   - è®¾ç½®æœ€å¤§åºåˆ—é•¿åº¦ï¼Œå¹³è¡¡æ€§èƒ½å’Œå†…å­˜ä½¿ç”¨

5. `--enforce-eager`:
   - å¼ºåˆ¶ä½¿ç”¨eageræ¨¡å¼ï¼Œé¿å…CUDAå›¾å½¢ç¼–è¯‘å¼€é”€

è¿™äº›å‚æ•°ç¡®ä¿äº†åœ¨RTX 3090 24Gç¯å¢ƒä¸‹èƒ½å¤Ÿç¨³å®šè¿è¡Œï¼Œå¹¶æä¾›è‰¯å¥½çš„å¹¶å‘å¤„ç†èƒ½åŠ›ã€‚

## ğŸ§ª æ¨¡å‹æ¨ç†ç¤ºä¾‹

### Python APIæ¨ç†
```python
import requests
import json

# é…ç½®
VLLM_HOST = "http://localhost:8000"
MODEL_NAME = "Qwen/Qwen2.5-0.5B-Instruct"

def generate_response(prompt, max_tokens=200, temperature=0.8):
    """ç”Ÿæˆå›å¤"""
    request_data = {
        "model": MODEL_NAME,
        "prompt": prompt,
        "max_tokens": max_tokens,
        "temperature": temperature,
        "top_p": 0.9,
        "top_k": 40
    }
    
    response = requests.post(
        f"{VLLM_HOST}/v1/completions",
        json=request_data,
        timeout=30
    )
    
    if response.status_code == 200:
        result = response.json()
        return result['choices'][0]['text'] if result.get('choices') else ""
    else:
        raise Exception(f"è¯·æ±‚å¤±è´¥: {response.status_code} - {response.text}")

# ç¤ºä¾‹ä½¿ç”¨
if __name__ == "__main__":
    response = generate_response("ä½ å¥½ï¼Œç”„å¬›ï¼Œä»Šå¤©å¤©æ°”å¦‚ä½•ï¼Ÿ")
    print(response)
```

### æ‰¹é‡æ¨ç†
```python
import requests
import json
from concurrent.futures import ThreadPoolExecutor, as_completed
import psutil

VLLM_HOST = "http://localhost:8000"
MODEL_NAME = "Qwen/Qwen2.5-0.5B-Instruct"

# æ ¹æ®CPUæ ¸å¿ƒæ•°åŠ¨æ€è®¾ç½®å¹¶å‘å‚æ•°
cpu_count = psutil.cpu_count(logical=False) or 4
MAX_CONCURRENT_REQUESTS = min(8, max(1, cpu_count // 2))

def generate_response(prompt_data):
    """ç”Ÿæˆå•ä¸ªå›å¤"""
    prompt, max_tokens, temperature = prompt_data
    request_data = {
        "model": MODEL_NAME,
        "prompt": prompt,
        "max_tokens": max_tokens,
        "temperature": temperature,
        "top_p": 0.9,
        "top_k": 40
    }
    
    try:
        response = requests.post(
            f"{VLLM_HOST}/v1/completions",
            json=request_data,
            timeout=30
        )
        response.raise_for_status()
        result = response.json()
        return result['choices'][0]['text'] if result.get('choices') else ""
    except Exception as e:
        return f"é”™è¯¯: {str(e)}"

def batch_generate(prompts_data):
    """æ‰¹é‡ç”Ÿæˆå›å¤"""
    results = []
    with ThreadPoolExecutor(max_workers=MAX_CONCURRENT_REQUESTS) as executor:
        # æäº¤æ‰€æœ‰ä»»åŠ¡
        future_to_prompt = {
            executor.submit(generate_response, prompt_data): prompt_data 
            for prompt_data in prompts_data
        }
        
        # æ”¶é›†ç»“æœ
        for future in as_completed(future_to_prompt):
            prompt_data = future_to_prompt[future]
            try:
                result = future.result()
                results.append((prompt_data[0], result))  # (prompt, response)
            except Exception as e:
                results.append((prompt_data[0], f"é”™è¯¯: {str(e)}"))
    
    return results

# ç¤ºä¾‹ä½¿ç”¨
if __name__ == "__main__":
    prompts = [
        ("çš‡ä¸Šï¼Œæ‚¨ä»Šæ—¥æ€ä¹ˆè¿™ä¹ˆé«˜å…´ï¼Ÿ", 200, 0.8),
        ("åå¦ƒå¾…æˆ‘å¦‚ä½•ï¼Ÿ", 150, 0.7),
        ("è¯·ä¸ºæˆ‘ä½œä¸€é¦–è¯—", 300, 0.9),
        ("ä½ æœ€å–œæ¬¢ä»€ä¹ˆèŠ±ï¼Ÿ", 100, 0.7)
    ]
    
    results = batch_generate(prompts)
    for prompt, response in results:
        print(f"é—®é¢˜: {prompt}")
        print(f"å›ç­”: {response}\n")
```

## ğŸ› ï¸ æ€§èƒ½ä¼˜åŒ–å»ºè®®

### 1. GPUå†…å­˜ä¼˜åŒ–
```bash
# è®¾ç½®GPUå†…å­˜ä½¿ç”¨æ¯”ä¾‹
--gpu-memory-utilization 0.8

# è®¾ç½®æœ€å¤§æ¨¡å‹é•¿åº¦
--max-model-len 4096

# è®¾ç½®å¼ é‡å¹¶è¡Œå¤§å°ï¼ˆå¤šGPUï¼‰
--tensor-parallel-size 1
```

### 2. æ¨ç†å‚æ•°ä¼˜åŒ–
```python
request_data = {
    "temperature": 0.8,        # æ§åˆ¶è¾“å‡ºéšæœºæ€§
    "top_p": 0.9,              # æ ¸é‡‡æ ·
    "top_k": 40,               # top-ké‡‡æ ·
    "max_tokens": 512          # æœ€å¤§ç”Ÿæˆtokenæ•°
}
```

### 3. LoRAé…ç½®ä¼˜åŒ–
```bash
# è®¾ç½®æœ€å¤§LoRA rank
--max-lora-rank 64

# å¯ç”¨LoRA
--enable-lora
```

## ğŸ§¾ æ³¨æ„äº‹é¡¹

1. ç¡®ä¿åŸºç¡€æ¨¡å‹ Qwen/Qwen2.5-0.5B-Instruct å·²ä¸‹è½½æˆ–å¯è®¿é—®
2. LoRAé€‚é…å™¨è·¯å¾„å¿…é¡»æ­£ç¡®
3. GPUå†…å­˜è‡³å°‘éœ€è¦8GBä»¥ä¸Š
4. æ ¹æ®å®é™…ç¡¬ä»¶é…ç½®è°ƒæ•´å‚æ•°
5. å¦‚æœä½¿ç”¨Dockerï¼Œè¯·ç¡®ä¿å·²å®‰è£…NVIDIA Container Toolkit
6. å¦‚é‡åˆ°CUDAç›¸å…³é”™è¯¯ï¼Œå¯èƒ½éœ€è¦æŒ‡å®šæ­£ç¡®çš„CUDAç‰ˆæœ¬

## ğŸ“ APIè°ƒç”¨ç¤ºä¾‹

### å•æ¬¡å¯¹è¯
```bash
curl -X POST http://localhost:8000/v1/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "Qwen/Qwen2.5-0.5B-Instruct",
    "prompt": "ä½ å¥½ï¼Œç”„å¬›",
    "max_tokens": 300,
    "temperature": 0.8
  }'
```

### èŠå¤©æ¥å£ï¼ˆå¦‚æœä½¿ç”¨OpenAIå…¼å®¹APIï¼‰
```bash
curl -X POST http://localhost:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "Qwen/Qwen2.5-0.5B-Instruct",
    "messages": [
      {
        "role": "system",
        "content": "ä½ æ˜¯ç”„å¬›ï¼Œã€Šç”„å¬›ä¼ ã€‹ä¸­çš„å¥³ä¸»è§’..."
      },
      {
        "role": "user",
        "content": "çš‡ä¸Šï¼Œæ‚¨ä»Šæ—¥æ€ä¹ˆè¿™ä¹ˆé«˜å…´ï¼Ÿ"
      }
    ],
    "max_tokens": 300,
    "temperature": 0.8
  }'
```