# ç”„å¬›æ¨¡å‹ Docker vLLM éƒ¨ç½²æŒ‡å—

åŸºäº Qwen2.5-0.5B + LoRA å¾®è°ƒçš„ç”„å¬›è§’è‰²æ¨¡å‹ Docker vLLM éƒ¨ç½²æŒ‡å—

## ğŸ“‹ å‰ææ¡ä»¶

### 1. ç¡®è®¤è®­ç»ƒå®Œæˆ
ç¡®ä¿ä»¥ä¸‹æ–‡ä»¶å­˜åœ¨ï¼š
```
training/models/huanhuan_fast/
â”œâ”€â”€ adapter_config.json
â”œâ”€â”€ adapter_model.safetensors
â””â”€â”€ train_results.json
```

### 2. å®‰è£… Docker å’Œ NVIDIA Container Toolkit
```bash
# Ubuntuå®‰è£…Docker
sudo apt update
sudo apt install docker.io

# å®‰è£…NVIDIA Container Toolkit
distribution=$(. /etc/os-release;echo $ID$VERSION_ID)
curl -s -L https://nvidia.github.io/libnvidia-container/gpgkey | sudo apt-key add -
curl -s -L https://nvidia.github.io/libnvidia-container/$distribution/libnvidia-container.list | sudo tee /etc/apt/sources.list.d/nvidia-container-toolkit.list

sudo apt update
sudo apt install -y nvidia-container-toolkit

# é‡å¯DockeræœåŠ¡
sudo systemctl restart docker
```

### 3. éªŒè¯ç¯å¢ƒ
```bash
# éªŒè¯Dockerå®‰è£…
docker --version

# éªŒè¯NVIDIA Dockeræ”¯æŒ
docker run --rm --gpus all nvidia/cuda:11.0-base-ubuntu20.04 nvidia-smi
```

## ğŸ³ Docker vLLM éƒ¨ç½²æ–¹å¼

### 1. å‡†å¤‡æ¨¡å‹æ–‡ä»¶
å°†è®­ç»ƒå¥½çš„LoRAæ¨¡å‹æ–‡ä»¶è½¬æ¢ä¸ºvLLMå…¼å®¹æ ¼å¼ï¼š
```bash
# ç¡®ä¿æ¨¡å‹æ–‡ä»¶åœ¨æŒ‡å®šä½ç½®
ls training/models/huanhuan_fast/

# å¦‚æœè¿˜æ²¡æœ‰è½¬æ¢æ¨¡å‹ï¼Œæ‰§è¡Œè½¬æ¢
python convert_lora_to_gguf.py
```

### 2. æ‹‰å–vLLM Dockeré•œåƒ
```bash
# æ‹‰å–vLLMå®˜æ–¹é•œåƒ
docker pull vllm/vllm-openai:latest
```

### 3. å¯åŠ¨Dockerå®¹å™¨
```bash
# è¿è¡ŒvLLMå®¹å™¨ (é€‚ç”¨äºRTX 3090 24Gç¯å¢ƒ)
docker run --gpus all \
    --name huanhuan-vllm \
    -v $(pwd)/training/models/huanhuan_fast:/models/huanhuan_fast \
    -v $(pwd)/deployment:/deployment \
    -p 8000:8000 \
    --env NCCL_IGNORE_DISABLED_P2P=1 \
    vllm/vllm-openai:latest \
    --model Qwen/Qwen2.5-0.5B-Instruct \
    --adapter /models/huanhuan_fast \
    --host 0.0.0.0 \
    --port 8000 \
    --enable-lora \
    --max-lora-rank 16 \
    --tensor-parallel-size 1 \
    --gpu-memory-utilization 0.8 \
    --max-model-len 4096 \
    --enforce-eager \
    --max-num-seqs 256 \
    --max-num-batched-tokens 4096
```

### 4. éªŒè¯éƒ¨ç½²
```bash
# æµ‹è¯•API
curl -X POST http://localhost:8000/v1/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "Qwen/Qwen2.5-0.5B-Instruct",
    "prompt": "ä½ æ˜¯è°ï¼Ÿ",
    "max_tokens": 200
  }'
```

## âš¡ å¿«é€Ÿéƒ¨ç½²æ–¹å¼ (é€‚ç”¨äºAutodlå®ä¾‹)

### 1. å®‰è£…ä¾èµ–
```bash
# æ¿€æ´»è™šæ‹Ÿç¯å¢ƒ
source /root/autodl-tmp/huanhuan_env/bin/activate

# å®‰è£…vLLm
pip install vllm==0.5.4

# éªŒè¯å®‰è£…
python -c "import vllm; print(vllm.__version__)"
```

### 2. å¯åŠ¨vLLMæœåŠ¡
```bash
# ç¡®ä¿åœ¨é¡¹ç›®æ ¹ç›®å½•
cd /path/to/ollama_baseline

# å¯åŠ¨vLLMæœåŠ¡ (é’ˆå¯¹RTX 3090 24Gé…ç½®ä¼˜åŒ–)
python -m vllm.entrypoints.openai.api_server \
    --model Qwen/Qwen2.5-0.5B-Instruct \
    --adapter ./training/models/huanhuan_fast \
    --host 0.0.0.0 \
    --port 8000 \
    --enable-lora \
    --max-lora-rank 16 \
    --tensor-parallel-size 1 \
    --gpu-memory-utilization 0.8 \
    --max-model-len 4096 \
    --enforce-eager \
    --max-num-seqs 256 \
    --max-num-batched-tokens 4096
```

## âš™ï¸ å‚æ•°è¯¦ç»†è¯´æ˜

### æ ¸å¿ƒæ¨¡å‹å‚æ•°
1. `--model Qwen/Qwen2.5-0.5B-Instruct`: æŒ‡å®šåŸºç¡€æ¨¡å‹åç§°æˆ–è·¯å¾„
2. `--adapter ./training/models/huanhuan_fast`: æŒ‡å®šLoRAé€‚é…å™¨è·¯å¾„
3. `--host 0.0.0.0`: æŒ‡å®šæœåŠ¡ç›‘å¬åœ°å€ï¼Œ0.0.0.0è¡¨ç¤ºç›‘å¬æ‰€æœ‰ç½‘ç»œæ¥å£
4. `--port 8000`: æŒ‡å®šæœåŠ¡ç›‘å¬ç«¯å£
5. `--enable-lora`: å¯ç”¨LoRAé€‚é…å™¨æ”¯æŒ
6. `--max-lora-rank 16`: è®¾ç½®LoRAé€‚é…å™¨çš„æœ€å¤§ç§©(rank)ï¼Œåº”ç•¥å¤§äºå®é™…ä½¿ç”¨çš„rankå€¼

### ç¡¬ä»¶èµ„æºé…ç½®å‚æ•°
1. `--tensor-parallel-size 1`: è®¾ç½®å¼ é‡å¹¶è¡Œå¤§å°ï¼Œå•GPUè®¾ç½®ä¸º1
2. `--gpu-memory-utilization 0.8`: è®¾ç½®GPUå†…å­˜ä½¿ç”¨ç‡ä¸Šé™ï¼Œ0.8è¡¨ç¤ºä½¿ç”¨80%çš„GPUå†…å­˜
3. `--max-model-len 4096`: è®¾ç½®æ¨¡å‹æœ€å¤§åºåˆ—é•¿åº¦
4. `--enforce-eager`: å¼ºåˆ¶ä½¿ç”¨eageræ¨¡å¼æ‰§è¡Œï¼Œè€Œéç¼–è¯‘æ¨¡å¼

### å¹¶å‘æ§åˆ¶å‚æ•°
1. `--max-num-seqs 256`: è®¾ç½®æ¯ä¸ªè¿­ä»£ä¸­å¤„ç†çš„æœ€å¤§åºåˆ—æ•°ï¼Œæ§åˆ¶å¹¶å‘è¯·æ±‚æ•°é‡
2. `--max-num-batched-tokens 4096`: è®¾ç½®æ¯ä¸ªè¿­ä»£ä¸­å¤„ç†çš„æœ€å¤§tokenæ•°ï¼Œæ§åˆ¶æ‰¹å¤„ç†å¤§å°

è¿™äº›å¹¶å‘æ§åˆ¶å‚æ•°ç”¨äºè°ƒèŠ‚vLLMçš„æ‰¹å¤„ç†èƒ½åŠ›å’Œå¹¶å‘æ€§èƒ½ï¼š
- `--max-num-seqs`: æ§åˆ¶åŒæ—¶å¤„ç†çš„æœ€å¤§è¯·æ±‚æ•°ï¼Œæ•°å€¼è¶Šå¤§å¯ä»¥å¤„ç†æ›´å¤šå¹¶å‘è¯·æ±‚ï¼Œä½†ä¼šå¢åŠ å†…å­˜æ¶ˆè€—
- `--max-num-batched-tokens`: æ§åˆ¶æ¯ä¸ªæ‰¹æ¬¡ä¸­å¤„ç†çš„æœ€å¤§tokenæ•°ï¼Œæ•°å€¼è¶Šå¤§æ‰¹å¤„ç†æ•ˆç‡è¶Šé«˜ï¼Œä½†éœ€è¦æ›´å¤šæ˜¾å­˜

## ğŸ§ª æ¨¡å‹æ¨ç†ç¤ºä¾‹

### Python APIæ¨ç†
```
import requests
import json

# é…ç½®
VLLM_HOST = "http://localhost:8000"
MODEL_NAME = "Qwen/Qwen2.5-0.5B-Instruct"

def generate_response(prompt, max_tokens=200, temperature=0.8):
    """ç”Ÿæˆå›å¤"""
    request_data = {
        "model": MODEL_NAME,
        "prompt": prompt,
        "max_tokens": max_tokens,
        "temperature": temperature,
        "top_p": 0.9,
        "top_k": 40
    }
    
    response = requests.post(
        f"{VLLM_HOST}/v1/completions",
        json=request_data,
        timeout=30
    )
    
    if response.status_code == 200:
        result = response.json()
        return result['choices'][0]['text'] if result.get('choices') else ""
    else:
        raise Exception(f"è¯·æ±‚å¤±è´¥: {response.status_code} - {response.text}")

# ç¤ºä¾‹ä½¿ç”¨
if __name__ == "__main__":
    response = generate_response("ä½ å¥½ï¼Œç”„å¬›ï¼Œä»Šå¤©å¤©æ°”å¦‚ä½•ï¼Ÿ")
    print(response)
```

### æ‰¹é‡æ¨ç†
```
import requests
import json
from concurrent.futures import ThreadPoolExecutor, as_completed
import psutil

VLLM_HOST = "http://localhost:8000"
MODEL_NAME = "Qwen/Qwen2.5-0.5B-Instruct"

# æ ¹æ®CPUæ ¸å¿ƒæ•°åŠ¨æ€è®¾ç½®å¹¶å‘å‚æ•°
cpu_count = psutil.cpu_count(logical=False) or 4
MAX_CONCURRENT_REQUESTS = min(8, max(1, cpu_count // 2))

def generate_response(prompt_data):
    """ç”Ÿæˆå•ä¸ªå›å¤"""
    prompt, max_tokens, temperature = prompt_data
    request_data = {
        "model": MODEL_NAME,
        "prompt": prompt,
        "max_tokens": max_tokens,
        "temperature": temperature,
        "top_p": 0.9,
        "top_k": 40
    }
    
    try:
        response = requests.post(
            f"{VLLM_HOST}/v1/completions",
            json=request_data,
            timeout=30
        )
        response.raise_for_status()
        result = response.json()
        return result['choices'][0]['text'] if result.get('choices') else ""
    except Exception as e:
        return f"é”™è¯¯: {str(e)}"

def batch_generate(prompts_data):
    """æ‰¹é‡ç”Ÿæˆå›å¤"""
    results = []
    with ThreadPoolExecutor(max_workers=MAX_CONCURRENT_REQUESTS) as executor:
        # æäº¤æ‰€æœ‰ä»»åŠ¡
        future_to_prompt = {
            executor.submit(generate_response, prompt_data): prompt_data 
            for prompt_data in prompts_data
        }
        
        # æ”¶é›†ç»“æœ
        for future in as_completed(future_to_prompt):
            prompt_data = future_to_prompt[future]
            try:
                result = future.result()
                results.append((prompt_data[0], result))  # (prompt, response)
            except Exception as e:
                results.append((prompt_data[0], f"é”™è¯¯: {str(e)}"))
    
    return results

# ç¤ºä¾‹ä½¿ç”¨
if __name__ == "__main__":
    prompts = [
        ("çš‡ä¸Šï¼Œæ‚¨ä»Šæ—¥æ€ä¹ˆè¿™ä¹ˆé«˜å…´ï¼Ÿ", 200, 0.8),
        ("åå¦ƒå¾…æˆ‘å¦‚ä½•ï¼Ÿ", 150, 0.7),
        ("è¯·ä¸ºæˆ‘ä½œä¸€é¦–è¯—", 300, 0.9),
        ("ä½ æœ€å–œæ¬¢ä»€ä¹ˆèŠ±ï¼Ÿ", 100, 0.7)
    ]
    
    results = batch_generate(prompts)
    for prompt, response in results:
        print(f"é—®é¢˜: {prompt}")
        print(f"å›ç­”: {response}\n")
```

## ğŸ› ï¸ æ€§èƒ½ä¼˜åŒ–å»ºè®®

### 1. GPUå†…å­˜ä¼˜åŒ–
```
# è®¾ç½®GPUå†…å­˜ä½¿ç”¨æ¯”ä¾‹
--gpu-memory-utilization 0.8

# è®¾ç½®æœ€å¤§æ¨¡å‹é•¿åº¦
--max-model-len 4096

# è®¾ç½®å¼ é‡å¹¶è¡Œå¤§å°ï¼ˆå¤šGPUï¼‰
--tensor-parallel-size 1
```

### 2. æ¨ç†å‚æ•°ä¼˜åŒ–
```
request_data = {
    "temperature": 0.8,        # æ§åˆ¶è¾“å‡ºéšæœºæ€§
    "top_p": 0.9,              # æ ¸é‡‡æ ·
    "top_k": 40,               # top-ké‡‡æ ·
    "max_tokens": 512          # æœ€å¤§ç”Ÿæˆtokenæ•°
}
```

### 3. LoRAé…ç½®ä¼˜åŒ–
```
# è®¾ç½®æœ€å¤§LoRA rank
--max-lora-rank 64

# å¯ç”¨LoRA
--enable-lora
```

## ğŸ§¾ æ³¨æ„äº‹é¡¹

1. ç¡®ä¿åŸºç¡€æ¨¡å‹ Qwen/Qwen2.5-0.5B-Instruct å·²ä¸‹è½½æˆ–å¯è®¿é—®
2. LoRAé€‚é…å™¨è·¯å¾„å¿…é¡»æ­£ç¡®
3. GPUå†…å­˜è‡³å°‘éœ€è¦8GBä»¥ä¸Š
4. æ ¹æ®å®é™…ç¡¬ä»¶é…ç½®è°ƒæ•´å‚æ•°
5. å¦‚æœä½¿ç”¨Dockerï¼Œè¯·ç¡®ä¿å·²å®‰è£…NVIDIA Container Toolkit
6. å¦‚é‡åˆ°CUDAç›¸å…³é”™è¯¯ï¼Œå¯èƒ½éœ€è¦æŒ‡å®šæ­£ç¡®çš„CUDAç‰ˆæœ¬

## ğŸ“ APIè°ƒç”¨ç¤ºä¾‹

### å•æ¬¡å¯¹è¯
```
curl -X POST http://localhost:8000/v1/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "Qwen/Qwen2.5-0.5B-Instruct",
    "prompt": "ä½ å¥½ï¼Œç”„å¬›",
    "max_tokens": 300,
    "temperature": 0.8
  }'
```

### èŠå¤©æ¥å£ï¼ˆå¦‚æœä½¿ç”¨OpenAIå…¼å®¹APIï¼‰
```
curl -X POST http://localhost:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "Qwen/Qwen2.5-0.5B-Instruct",
    "messages": [
      {
        "role": "system",
        "content": "ä½ æ˜¯ç”„å¬›ï¼Œã€Šç”„å¬›ä¼ ã€‹ä¸­çš„å¥³ä¸»è§’..."
      },
      {
        "role": "user",
        "content": "çš‡ä¸Šï¼Œæ‚¨ä»Šæ—¥æ€ä¹ˆè¿™ä¹ˆé«˜å…´ï¼Ÿ"
      }
    ],
    "max_tokens": 300,
    "temperature": 0.8
  }'
```