#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
é¡¹ç›®ä¸€é”®è¿è¡Œè„šæœ¬

è¯¥è„šæœ¬æŒ‰é¡ºåºæ‰§è¡Œä»¥ä¸‹æ“ä½œï¼š
1. ä¸‹è½½æ•°æ®
2. å¤„ç†æ•°æ®
3. å¾®è°ƒæ¨¡å‹
4. è½¬æ¢æ¨¡å‹æ ¼å¼
5. è¯„ä¼°æ¨¡å‹
6. éƒ¨ç½²æ¨¡å‹
7. å¯åŠ¨Webç•Œé¢
8. å¯åŠ¨MCPæœåŠ¡å™¨

é€‚ç”¨äºLinux Ubuntuç³»ç»Ÿï¼ŒRTX 3090æ˜¾å¡ï¼ŒAutoDLæœåŠ¡å™¨å®ä¾‹

ä½¿ç”¨è¯´æ˜:
========
1. åŸºæœ¬ç”¨æ³•:
   python main_run.py

2. è·³è¿‡æŸäº›æ­¥éª¤:
   python main_run.py --skip-steps 1,2,3  # è·³è¿‡æ­¥éª¤1,2,3

3. åªè¿è¡Œç‰¹å®šæ­¥éª¤:
   python main_run.py --only-step 4      # åªè¿è¡Œæ­¥éª¤4

4. é…ç½®åŒ–å‚æ•°ä½¿ç”¨:
   python main_run.py --config config.yaml  # ä½¿ç”¨è‡ªå®šä¹‰é…ç½®æ–‡ä»¶

5. æŸ¥çœ‹å¸®åŠ©:
   python main_run.py --help

é…ç½®æ–‡ä»¶è¯´æ˜:
===========
å¯ä»¥é€šè¿‡YAMLé…ç½®æ–‡ä»¶è‡ªå®šä¹‰å„ä¸ªæ¨¡å—çš„å‚æ•°ï¼Œç¤ºä¾‹é…ç½®å¦‚ä¸‹:

data_download:
  source_url: "https://raw.githubusercontent.com/datawhalechina/self-llm/master/dataset"
  data_file: "huanhuan.json"

data_process:
  max_samples: null  # nullè¡¨ç¤ºå¤„ç†å…¨éƒ¨æ•°æ®
  train_ratio: 0.8
  val_ratio: 0.1
  test_ratio: 0.1

model_training:
  config_file: "training/huanhuan_config_fast.yaml"
  base_model: "qwen2.5:0.5b"
  output_dir: "training/models/huanhuan_fast"

model_deployment:
  base_model: "qwen2.5:0.5b"
  modelfile: "deployment/Modelfile.huanhuan"
  model_name: "huanhuan_fast"

web_interface:
  port: 8501

mcp_server:
  port: 3141
"""

import os
import sys
import subprocess
import argparse
import time
import yaml
from pathlib import Path
from typing import Dict, Any, Optional

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
script_dir = Path(__file__).parent
project_root = script_dir.parent
sys.path.insert(0, str(project_root))
    
# å¯¼å…¥typingæ¨¡å—çš„Listç±»å‹ç”¨äºç±»å‹æ³¨è§£
from typing import List

# é»˜è®¤é…ç½®
DEFAULT_CONFIG = {
    "data_download": {
        "source_url": "https://raw.githubusercontent.com/datawhalechina/self-llm/master/dataset",
        "data_file": "huanhuan.json"
    },
    "data_process": {
        "max_samples": 50,
        "train_ratio": 0.8,
        "val_ratio": 0.1,
        "test_ratio": 0.1
    },
    "model_training": {
        "config_file": "training/huanhuan_config_fast.yaml",
        "base_model": "qwen2.5:0.5b",
        "output_dir": "training/models/huanhuan_fast"
    },
    "model_deployment": {
        "base_model": "qwen2.5:0.5b",
        "modelfile": "deployment/Modelfile.huanhuan",
        "model_name": "huanhuan_fast"
    },
    "web_interface": {
        "port": 8501
    },
    "mcp_server": {
        "port": 3141
    }
}

def load_config(config_path: Optional[str] = None) -> Dict[str, Any]:
    """
    åŠ è½½é…ç½®æ–‡ä»¶
    
    Args:
        config_path (str, optional): é…ç½®æ–‡ä»¶è·¯å¾„
        
    Returns:
        Dict[str, Any]: é…ç½®å­—å…¸
    """
    config = DEFAULT_CONFIG.copy()
    
    if config_path and Path(config_path).exists():
        with open(config_path, 'r', encoding='utf-8') as f:
            user_config = yaml.safe_load(f)
            # åˆå¹¶ç”¨æˆ·é…ç½®å’Œé»˜è®¤é…ç½®
            for key, value in user_config.items():
                if key in config:
                    config[key].update(value)
                else:
                    config[key] = value
    
    return config

def run_command(command, description, cwd=None, timeout=None):
    """
    è¿è¡Œå‘½ä»¤å¹¶æ˜¾ç¤ºè¾“å‡º
    
    Args:
        command (str): è¦æ‰§è¡Œçš„å‘½ä»¤
        description (str): å‘½ä»¤æè¿°
        cwd (str): å·¥ä½œç›®å½•ï¼Œé»˜è®¤ä¸ºNoneï¼ˆå½“å‰ç›®å½•ï¼‰
        timeout (int): è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰ï¼Œé»˜è®¤ä¸ºNone
    
    Returns:
        bool: å‘½ä»¤æ‰§è¡Œæ˜¯å¦æˆåŠŸ
    """
    print(f"\n{'='*60}")
    print(f"æ­£åœ¨æ‰§è¡Œ: {description}")
    print(f"å‘½ä»¤: {command}")
    print(f"å·¥ä½œç›®å½•: {cwd or 'å½“å‰ç›®å½•'}")
    print(f"{'='*60}")
    
    try:
        result = subprocess.run(
            command, 
            shell=True, 
            check=True, 
            text=True, 
            cwd=cwd,
            stdout=subprocess.PIPE,
            stderr=subprocess.STDOUT,
            timeout=timeout
        )
        print(result.stdout)
        print(f"âœ… {description} å®Œæˆ")
        return True
    except subprocess.CalledProcessError as e:
        print(f"âŒ {description} å¤±è´¥")
        print(f"é”™è¯¯ä¿¡æ¯: {e}")
        print(f"è¾“å‡º: {e.output}")
        return False
    except subprocess.TimeoutExpired:
        print(f"âŒ {description} è¶…æ—¶")
        return False
    except Exception as e:
        print(f"âŒ æ‰§è¡Œ {description} æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}")
        return False

def check_prerequisites():
    """
    æ£€æŸ¥è¿è¡Œç¯å¢ƒå’Œå‰ææ¡ä»¶
    """
    print("ğŸ” æ£€æŸ¥è¿è¡Œç¯å¢ƒ...")
    
    # æ£€æŸ¥Pythonç‰ˆæœ¬
    if sys.version_info < (3, 8):
        print("âŒ Pythonç‰ˆæœ¬å¿…é¡» >= 3.8")
        return False
    
    # æ£€æŸ¥CUDAæ˜¯å¦å¯ç”¨
    try:
        import torch
        if torch.cuda.is_available():
            print(f"âœ… CUDAå¯ç”¨ï¼ŒGPU: {torch.cuda.get_device_name()}")
        else:
            print("âš ï¸  CUDAä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨CPUè®­ç»ƒï¼ˆé€Ÿåº¦è¾ƒæ…¢ï¼‰")
    except ImportError:
        print("âŒ æœªå®‰è£…PyTorch")
        return False
    
    # æ£€æŸ¥Dockeræ˜¯å¦å®‰è£…
    try:
        result = subprocess.run(
            "docker --version", 
            shell=True, 
            check=True, 
            capture_output=True, 
            text=True
        )
        print(f"âœ… Dockerå·²å®‰è£…: {result.stdout.strip()}")
    except subprocess.CalledProcessError:
        print("âŒ Dockeræœªå®‰è£…ï¼Œè¯·å…ˆå®‰è£…Docker")
        print("   Ubuntuå®‰è£…å‘½ä»¤: sudo apt install docker.io")
        return False
    except FileNotFoundError:
        print("âŒ Dockeræœªå®‰è£…ï¼Œè¯·å…ˆå®‰è£…Docker")
        print("   Ubuntuå®‰è£…å‘½ä»¤: sudo apt install docker.io")
        return False
    
    print("âœ… ç¯å¢ƒæ£€æŸ¥é€šè¿‡")
    return True

def setup_environment():
    """
    è®¾ç½®ç¯å¢ƒå˜é‡
    """
    print("ğŸ”§ è®¾ç½®ç¯å¢ƒå˜é‡...")
    os.environ["TOKENIZERS_PARALLELISM"] = "false"
    os.environ["CUDA_VISIBLE_DEVICES"] = "0"  # ä½¿ç”¨ç¬¬ä¸€ä¸ªGPU
    print("âœ… ç¯å¢ƒå˜é‡è®¾ç½®å®Œæˆ")

def step1_download_data(config: Dict[str, Any]):
    """
    æ­¥éª¤1: ä¸‹è½½æ•°æ®
    
    Args:
        config (Dict[str, Any]): é…ç½®å­—å…¸
    """
    print("\nğŸ“¥ æ­¥éª¤1: ä¸‹è½½æ•°æ®")
    command = "python dataScripts/download_data.py"
    return run_command(command, "ä¸‹è½½æ•°æ®", cwd=project_root)

def step2_process_data(config: Dict[str, Any]):
    """
    æ­¥éª¤2: å¤„ç†æ•°æ®
    
    Args:
        config (Dict[str, Any]): é…ç½®å­—å…¸
    """
    print("\nğŸ§¹ æ­¥éª¤2: å¤„ç†æ•°æ®")
    # æ„å»ºå‘½ä»¤å‚æ•°
    max_samples = config["data_process"]["max_samples"]
    command = "python dataScripts/huanhuan_data_prepare.py"
    if max_samples is not None:
        command += f" {max_samples}"
    
    return run_command(command, "å¤„ç†æ•°æ®", cwd=project_root)

def step3_finetune_model(config: Dict[str, Any]):
    """
    æ­¥éª¤3: å¾®è°ƒæ¨¡å‹
    
    Args:
        config (Dict[str, Any]): é…ç½®å­—å…¸
    """
    print("\nğŸ¤– æ­¥éª¤3: å¾®è°ƒæ¨¡å‹")
    # ä½¿ç”¨é…ç½®æ–‡ä»¶è¿›è¡Œè®­ç»ƒ
    config_file = config["model_training"]["config_file"]
    # ç¡®ä¿ä½¿ç”¨ç›¸å¯¹äºé¡¹ç›®æ ¹ç›®å½•çš„è·¯å¾„
    config_file_path = project_root / config_file
    command = f"python training/huanhuan_train.py {config_file_path}"
    return run_command(command, "å¾®è°ƒæ¨¡å‹", cwd=project_root)

def step4_convert_model(config: Dict[str, Any]):
    """
    æ­¥éª¤4: è½¬æ¢æ¨¡å‹æ ¼å¼
    
    Args:
        config (Dict[str, Any]): é…ç½®å­—å…¸
    """
    print("\nğŸ”„ æ­¥éª¤4: è½¬æ¢æ¨¡å‹æ ¼å¼")
    # å…ˆæ£€æŸ¥æ˜¯å¦æœ‰è®­ç»ƒå¥½çš„æ¨¡å‹
    model_dir = project_root / config["model_training"]["output_dir"]
    if not model_dir.exists():
        print("âŒ æœªæ‰¾åˆ°è®­ç»ƒå¥½çš„æ¨¡å‹ï¼Œè¯·å…ˆå®Œæˆæ¨¡å‹è®­ç»ƒ")
        return False
    
    # æ£€æŸ¥æ˜¯å¦æœ‰é€‚é…å™¨æƒé‡
    adapter_path = model_dir / "adapter_model.safetensors"
    if not adapter_path.exists():
        print("âŒ æœªæ‰¾åˆ°é€‚é…å™¨æƒé‡æ–‡ä»¶")
        return False
    
    # è½¬æ¢LoRAæƒé‡ä¸ºGGUFæ ¼å¼
    command = f"python llama.cpp/convert_lora_to_gguf.py --outfile deployment/huanhuan_fast_lora.gguf {model_dir}"
    return run_command(command, "è½¬æ¢æ¨¡å‹æ ¼å¼", cwd=project_root)

def step5_evaluate_model(config: Dict[str, Any]):
    """
    æ­¥éª¤5: è¯„ä¼°æ¨¡å‹
    
    Args:
        config (Dict[str, Any]): é…ç½®å­—å…¸
    """
    print("\nğŸ“Š æ­¥éª¤5: è¯„ä¼°æ¨¡å‹")
    command = "python evaluate/example_usage.py"
    return run_command(command, "è¯„ä¼°æ¨¡å‹", cwd=project_root)

def step6_deploy_model(config: Dict[str, Any]):
    """
    æ­¥éª¤6: éƒ¨ç½²æ¨¡å‹ (ä½¿ç”¨vLLM)
    
    Args:
        config (Dict[str, Any]): é…ç½®å­—å…¸
    """
    print("\nğŸš€ æ­¥éª¤6: éƒ¨ç½²æ¨¡å‹ (ä½¿ç”¨vLLM)")
    if not config["model_deployment"]["enabled"]:
        print("â­ï¸  è·³è¿‡æ¨¡å‹éƒ¨ç½²æ­¥éª¤")
        return True
    
    # æ£€æŸ¥æ¨¡å‹æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    model_path = Path(config["model_deployment"]["model_path"])
    if not model_path.exists():
        print("âŒ æœªæ‰¾åˆ°è®­ç»ƒå¥½çš„æ¨¡å‹æ–‡ä»¶")
        return False
    
    # 1. æ‹‰å–vLLM Dockeré•œåƒ
    print("ğŸ“¥ æ‹‰å–vLLM Dockeré•œåƒ...")
    pull_command = "docker pull vllm/vllm-openai:latest"
    if not run_command(pull_command, "æ‹‰å–vLLM Dockeré•œåƒ"):
        print("âŒ æ‹‰å–vLLM Dockeré•œåƒå¤±è´¥")
        return False
    
    # 2. å¯åŠ¨vLLMæœåŠ¡
    print("ğŸ”„ å¯åŠ¨vLLMæœåŠ¡...")
    try:
        # æ£€æŸ¥æ˜¯å¦å·²æœ‰vLLMå®¹å™¨åœ¨è¿è¡Œ
        result = subprocess.run(
            "docker ps | grep huanhuan-vllm", 
            shell=True, 
            capture_output=True, 
            text=True
        )
        if result.returncode == 0:
            print("âœ… vLLMæœåŠ¡å·²åœ¨è¿è¡Œ")
        else:
            # å¯åŠ¨vLLMå®¹å™¨
            start_command = f"""docker run --gpus all \\
                --name huanhuan-vllm \\
                -v {project_root}/training/training/models/huanhuan_fast:/models/huanhuan_fast \\
                -p 8000:8000 \\
                --env NCCL_IGNORE_DISABLED_P2P=1 \\
                vllm/vllm-openai:latest \\
                --model Qwen/Qwen2.5-0.5B-Instruct \\
                --adapter /models/huanhuan_fast \\
                --host 0.0.0.0 \\
                --port 8000 \\
                --enable-lora \\
                --max-lora-rank 64 \\
                --tensor-parallel-size 1 \\
                --gpu-memory-utilization 0.8 \\
                --max-model-len 4096 \\
                --enforce-eager"""
            
            subprocess.Popen(
                start_command,
                shell=True,
                stdout=subprocess.DEVNULL,
                stderr=subprocess.DEVNULL
            )
            print("â³ ç­‰å¾…vLLMæœåŠ¡å¯åŠ¨...")
            time.sleep(15)  # ç­‰å¾…æœåŠ¡å¯åŠ¨
            
            # éªŒè¯æœåŠ¡æ˜¯å¦å¯åŠ¨æˆåŠŸ
            verify_command = "curl -f http://localhost:8000/v1/models"
            if run_command(verify_command, "éªŒè¯vLLMæœåŠ¡"):
                print("âœ… vLLMæœåŠ¡å¯åŠ¨æˆåŠŸ")
            else:
                print("âš ï¸  vLLMæœåŠ¡å¯åŠ¨å¯èƒ½å¤±è´¥ï¼Œè¯·æ‰‹åŠ¨æ£€æŸ¥")
    except Exception as e:
        print(f"âš ï¸  å¯åŠ¨vLLMæœåŠ¡æ—¶é‡åˆ°é—®é¢˜: {e}")
        print("   å°†å°è¯•ç»§ç»­æ‰§è¡Œåç»­æ­¥éª¤...")
    
    return True

def step7_start_web_interface(config: Dict[str, Any]):
    """
    æ­¥éª¤7: å¯åŠ¨Webç•Œé¢
    
    Args:
        config (Dict[str, Any]): é…ç½®å­—å…¸
    """
    print("\nğŸŒ æ­¥éª¤7: å¯åŠ¨Webç•Œé¢")
    port = config["web_interface"]["port"]
    print(f"ğŸ’¡ Webç•Œé¢å°†åœ¨åå°è¿è¡Œï¼Œè®¿é—®åœ°å€: http://localhost:{port}")
    command = f"streamlit run application/huanhuan_web.py --server.port {port}"
    
    # åœ¨åå°è¿è¡ŒWebç•Œé¢
    try:
        subprocess.Popen(
            command,
            shell=True,
            cwd=project_root,
            stdout=subprocess.DEVNULL,
            stderr=subprocess.DEVNULL
        )
        print("âœ… Webç•Œé¢å·²åœ¨åå°å¯åŠ¨")
        return True
    except Exception as e:
        print(f"âŒ å¯åŠ¨Webç•Œé¢å¤±è´¥: {e}")
        return False

def step8_start_mcp_server(config: Dict[str, Any]):
    """
    æ­¥éª¤8: å¯åŠ¨MCPæœåŠ¡å™¨
    
    Args:
        config (Dict[str, Any]): é…ç½®å­—å…¸
    """
    print("\nğŸ“¡ æ­¥éª¤8: å¯åŠ¨MCPæœåŠ¡å™¨")
    port = config["mcp_server"]["port"]
    print(f"ğŸ’¡ MCPæœåŠ¡å™¨å°†åœ¨åå°è¿è¡Œï¼Œç«¯å£: {port}")
    command = "python mcp_server/server.py"
    
    # åœ¨åå°è¿è¡ŒMCPæœåŠ¡å™¨
    try:
        subprocess.Popen(
            command,
            shell=True,
            cwd=project_root,
            stdout=subprocess.DEVNULL,
            stderr=subprocess.DEVNULL
        )
        print("âœ… MCPæœåŠ¡å™¨å·²åœ¨åå°å¯åŠ¨")
        return True
    except Exception as e:
        print(f"âŒ å¯åŠ¨MCPæœåŠ¡å™¨å¤±è´¥: {e}")
        return False

def main():
    """
    ä¸»å‡½æ•°ï¼šæŒ‰é¡ºåºæ‰§è¡Œæ‰€æœ‰æ­¥éª¤
    """
    print("ğŸ­ ç”„å¬›ä¼ è§’è‰²å¯¹è¯ç³»ç»Ÿ - ä¸€é”®è¿è¡Œè„šæœ¬")
    print("é€‚ç”¨äºLinux Ubuntuç³»ç»Ÿï¼ŒRTX 3090æ˜¾å¡ï¼ŒAutoDLæœåŠ¡å™¨å®ä¾‹")
    print(f"é¡¹ç›®æ ¹ç›®å½•: {project_root}")
    
    parser = argparse.ArgumentParser(
        description="ç”„å¬›ä¼ è§’è‰²å¯¹è¯ç³»ç»Ÿä¸€é”®è¿è¡Œè„šæœ¬",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog=__doc__
    )
    parser.add_argument(
        "--config", 
        default="fast_run/config.yaml",
        help='é…ç½®æ–‡ä»¶è·¯å¾„ (é»˜è®¤: fast_run/config.yaml)'
    )
    parser.add_argument(
        "--skip-steps", 
        type=str, 
        help="è·³è¿‡çš„æ­¥éª¤ç¼–å·ï¼Œç”¨é€—å·åˆ†éš”ï¼Œä¾‹å¦‚: 1,2,3"
    )
    parser.add_argument(
        "--only-steps", 
        type=str, 
        help="åªæ‰§è¡ŒæŒ‡å®šæ­¥éª¤ï¼Œä¾‹å¦‚: 6,7"
    )
    
    args = parser.parse_args()
    
    # åŠ è½½é…ç½®
    config = load_config(args.config)
    
    # è§£æè·³è¿‡çš„æ­¥éª¤
    skip_steps = set()
    if args.skip_steps:
        try:
            skip_steps = {int(x.strip()) for x in args.skip_steps.split(",")}
        except ValueError:
            print("âŒ --skip-steps å‚æ•°æ ¼å¼é”™è¯¯ï¼Œè¯·ä½¿ç”¨æ•°å­—å¹¶ç”¨é€—å·åˆ†éš”")
            return False
    # è§£æåªæ‰§è¡Œçš„æ­¥éª¤
    only_steps = []
    if args.only_steps:
        try:
            only_steps = [int(x.strip()) for x in args.only_steps.split(",")]
        except ValueError:
            print("âŒ --only-steps å‚æ•°æ ¼å¼é”™è¯¯ï¼Œè¯·ä½¿ç”¨æ•°å­—å¹¶ç”¨é€—å·åˆ†éš”")
            return False
    
    # è§£æåªæ‰§è¡Œçš„æ­¥éª¤
    only_steps = []
    if args.only_steps:
        try:
            only_steps = [int(x) for x in args.only_steps.split(",")]
        except ValueError:
            print("âŒ --only-steps å‚æ•°æ ¼å¼é”™è¯¯ï¼Œè¯·ä½¿ç”¨æ•°å­—å¹¶ç”¨é€—å·åˆ†éš”")
            return False
    
    # æ£€æŸ¥å‰ææ¡ä»¶
    if not check_prerequisites():
        print("âŒ ç¯å¢ƒæ£€æŸ¥å¤±è´¥ï¼Œè¯·æ£€æŸ¥ä¾èµ–å®‰è£…")
        return False
    
    # è®¾ç½®ç¯å¢ƒ
    setup_environment()
    
    # å®šä¹‰æ‰€æœ‰æ­¥éª¤
    steps = [
        (1, "ä¸‹è½½æ•°æ®", step1_download_data),
        (2, "å¤„ç†æ•°æ®", step2_process_data),
        (3, "å¾®è°ƒæ¨¡å‹", step3_finetune_model),
        (4, "è½¬æ¢æ¨¡å‹æ ¼å¼", step4_convert_model),
        (5, "è¯„ä¼°æ¨¡å‹", step5_evaluate_model),
        (6, "éƒ¨ç½²æ¨¡å‹", step6_deploy_model),
        (7, "å¯åŠ¨Webç•Œé¢", step7_start_web_interface),
        (8, "å¯åŠ¨MCPæœåŠ¡å™¨", step8_start_mcp_server)
    ]
    
    # ç¡®å®šè¦æ‰§è¡Œçš„æ­¥éª¤
    steps_to_run = []
    if only_steps:
        steps_to_run = [step for step in steps if step[0] in only_steps]
    else:
        steps_to_run = [step for step in steps if step[0] not in skip_steps]
    
    # æ‰§è¡Œæ­¥éª¤
    start_time = time.time()
    success_count = 0
    
    for step_num, step_name, step_func in steps:
        # åˆ¤æ–­æ˜¯å¦åº”è¯¥æ‰§è¡Œæ­¤æ­¥éª¤
        if only_steps and step_num not in only_steps:
            print(f"\nâ­ï¸  è·³è¿‡æ­¥éª¤ {step_num}: {step_name}")
            success_count += 1
            continue
            
        if step_num in skip_steps:
            print(f"\nâ­ï¸  è·³è¿‡æ­¥éª¤ {step_num}: {step_name}")
            success_count += 1
            continue
        
        # æ‰§è¡Œæ­¥éª¤
        if step_func(config):
            success_count += 1
        else:
            print(f"\nâŒ æ­¥éª¤ {step_num}: {step_name} å¤±è´¥ï¼Œåœæ­¢æ‰§è¡Œåç»­æ­¥éª¤")
            break
    
    # æ€»ç»“
    end_time = time.time()
    duration = end_time - start_time
    
    print(f"\n{'='*60}")
    print("ğŸ“‹ æ‰§è¡Œæ€»ç»“:")
    print(f"   æˆåŠŸæ­¥éª¤: {success_count}/{len(steps_to_run)}")
    print(f"   æ€»è€—æ—¶: {duration:.2f} ç§’")
    
    if success_count == len(steps_to_run):
        web_port = config["web_interface"]["port"]
        mcp_port = config["mcp_server"]["port"]
        model_name = config["model_deployment"]["model_name"]
        
        print("ğŸ‰ æ‰€æœ‰æ­¥éª¤æ‰§è¡Œå®Œæˆï¼")
        print("\nğŸ’¡ ä½¿ç”¨è¯´æ˜:")
        print(f"   - Webç•Œé¢åœ°å€: http://localhost:{web_port}")
        print(f"   - MCPæœåŠ¡å™¨åœ°å€: http://localhost:{mcp_port}")
        print(f"   - vLLMæœåŠ¡åœ°å€: http://localhost:8000")
        print(f"   - æ¨¡å‹åç§°: {model_name}")
    else:
        print("âš ï¸  éƒ¨åˆ†æ­¥éª¤æ‰§è¡Œå¤±è´¥ï¼Œè¯·æ£€æŸ¥æ—¥å¿—")
    
    print(f"{'='*60}")

if __name__ == "__main__":
    main()