# ä½¿ç”¨vLLMéƒ¨ç½²ç”„å¬›è§’è‰²æ¨¡å‹æŒ‡å—

åŸºäº Qwen2.5-0.5B + LoRA å¾®è°ƒçš„ç”„å¬›è§’è‰²æ¨¡å‹vLLMéƒ¨ç½²æŒ‡å—

## ğŸ“‹ å‰ææ¡ä»¶

### 1. ç¡®è®¤è®­ç»ƒå®Œæˆ
ç¡®ä¿ä»¥ä¸‹æ–‡ä»¶å­˜åœ¨ï¼š
```
training/training/models/huanhuan_fast/
â”œâ”€â”€ adapter_config.json
â”œâ”€â”€ adapter_model.safetensors
â””â”€â”€ train_results.json
```

### 2. å®‰è£… vLLM
```bash
# åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ
conda create -n vllm-huanhuan python=3.10 -y
conda activate vllm-huanhuan

# å®‰è£… vLLM (æ¨è0.5.4ç‰ˆæœ¬ä»¥ç¡®ä¿å…¼å®¹æ€§)
pip install vllm==0.5.4

# æˆ–è€…å®‰è£…æœ€æ–°ç‰ˆæœ¬
pip install vllm

# éªŒè¯å®‰è£…
python -c "import vllm; print(vllm.__version__)"
```

## ğŸ³ Dockeréƒ¨ç½²æ–¹å¼

### 1. æ‹‰å–vLLM Dockeré•œåƒ
```bash
docker pull vllm/vllm-openai:latest
```

### 2. å‡†å¤‡æ¨¡å‹æ–‡ä»¶
å°†è®­ç»ƒå¥½çš„LoRAæ¨¡å‹æ–‡ä»¶å¤åˆ¶åˆ°é¡¹ç›®ç›®å½•ä¸­ï¼š
```bash
# ç¡®ä¿æ¨¡å‹æ–‡ä»¶åœ¨æŒ‡å®šä½ç½®
ls training/training/models/huanhuan_fast/
```

### 3. å¯åŠ¨Dockerå®¹å™¨
```bash
# è¿è¡ŒvLLMå®¹å™¨
docker run --gpus all \
    --name huanhuan-vllm \
    -v $(pwd)/training/training/models/huanhuan_fast:/models/huanhuan_fast \
    -v $(pwd)/deployment:/deployment \
    -p 8000:8000 \
    --env NCCL_IGNORE_DISABLED_P2P=1 \
    vllm/vllm-openai:latest \
    --model Qwen/Qwen2.5-0.5B-Instruct \
    --adapter /models/huanhuan_fast \
    --host 0.0.0.0 \
    --port 8000 \
    --enable-lora \
    --max-lora-rank 64 \
    --tensor-parallel-size 1
```

### 4. éªŒè¯éƒ¨ç½²
```bash
# æµ‹è¯•API
curl -X POST http://localhost:8000/v1/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "Qwen/Qwen2.5-0.5B-Instruct",
    "prompt": "ä½ æ˜¯è°ï¼Ÿ",
    "max_tokens": 200
  }'
```

## âš¡ å¿«é€Ÿéƒ¨ç½²æ–¹å¼

### 1. ç›´æ¥å‘½ä»¤è¡Œéƒ¨ç½²
```bash
# ç¡®ä¿åœ¨é¡¹ç›®æ ¹ç›®å½•
cd /path/to/ollama_baseline

# å¯åŠ¨vLLMæœåŠ¡
python -m vllm.entrypoints.api_server \
    --model Qwen/Qwen2.5-0.5B-Instruct \
    --adapter ./training/training/models/huanhuan_fast \
    --host 0.0.0.0 \
    --port 8000 \
    --enable-lora \
    --max-lora-rank 64
```

### 2. ä½¿ç”¨Pythonä»£ç éƒ¨ç½²
åˆ›å»º [deployment/vllm_server.py](file:///e:/program/python/ollama_baseline/deployment/vllm_server.py) æ–‡ä»¶ï¼š

```python
from vllm import LLM
from vllm.sampling_params import SamplingParams

# é…ç½®é‡‡æ ·å‚æ•°
sampling_params = SamplingParams(
    temperature=0.8,
    top_p=0.9,
    top_k=40,
    repetition_penalty=1.05,
    max_tokens=512
)

# åˆå§‹åŒ–æ¨¡å‹
llm = LLM(
    model="Qwen/Qwen2.5-0.5B-Instruct",
    enable_lora=True,
    max_lora_rank=64,
    dtype="auto"
)

# æ·»åŠ LoRAé€‚é…å™¨
llm.add_lora("./training/training/models/huanhuan_fast")

# ç³»ç»Ÿæç¤ºè¯
system_prompt = """ä½ æ˜¯ç”„å¬›ï¼Œã€Šç”„å¬›ä¼ ã€‹ä¸­çš„å¥³ä¸»è§’ã€‚ä½ æ˜¯å¤§ç†å¯ºå°‘å¿ç”„è¿œé“ä¹‹å¥³ï¼Œ
å› é€‰ç§€å…¥å®«ï¼Œåæˆä¸ºç†¹è´µå¦ƒã€‚ä½ èªæ…§æœºæ™ºï¼Œæ¸©å©‰è´¤æ·‘ï¼ŒçŸ¥ä¹¦è¾¾ç†ï¼Œ
æ“…é•¿è¯—è¯æ­Œèµ‹ã€‚è¯·ç”¨ç”„å¬›çš„è¯­æ°”å’Œé£æ ¼æ¥å›ç­”é—®é¢˜ï¼Œ
è¯­è¨€è¦å¤å…¸é›…è‡´ï¼Œè°¦é€Šæœ‰ç¤¼ï¼Œä½“ç°å‡ºå®«å»·å¥³å­çš„æ•™å…»å’Œæ™ºæ…§ã€‚

å›ç­”æ—¶è¯·æ³¨æ„ï¼š
1. ä½¿ç”¨"è‡£å¦¾"è‡ªç§°
2. è¯­è¨€è¦å…¸é›…ï¼Œå¤šç”¨"ä¾¿æ˜¯"ã€"å€’æ˜¯"ã€"åªæ˜¯"ç­‰å¤å…¸ç”¨è¯
3. ä½“ç°å‡ºæ¸©å©‰è´¤æ·‘çš„æ€§æ ¼ç‰¹ç‚¹
4. å¯ä»¥é€‚å½“æåŠå®«å»·ç”Ÿæ´»å’Œè¯—è¯æ–‡åŒ–
5. ä¿æŒè§’è‰²çš„ä¸€è‡´æ€§å’ŒçœŸå®æ€§"""

def generate_response(prompt):
    # æ„é€ å®Œæ•´çš„å¯¹è¯
    full_prompt = f"{system_prompt}\n\n{prompt}"
    
    # ç”Ÿæˆå“åº”
    outputs = llm.generate(full_prompt, sampling_params)
    return outputs[0].outputs[0].text

# ç¤ºä¾‹ä½¿ç”¨
if __name__ == "__main__":
    response = generate_response("çš‡ä¸Šï¼Œæ‚¨ä»Šæ—¥æ€ä¹ˆè¿™ä¹ˆé«˜å…´ï¼Ÿ")
    print(response)
```

è¿è¡ŒæœåŠ¡å™¨ï¼š
```bash
python deployment/vllm_server.py
```

### 3. OpenAIå…¼å®¹APIéƒ¨ç½²
```bash
# å¯åŠ¨OpenAIå…¼å®¹çš„APIæœåŠ¡å™¨
python -m vllm.entrypoints.openai.api_server \
    --model Qwen/Qwen2.5-0.5B-Instruct \
    --adapter ./training/training/models/huanhuan_fast \
    --host 0.0.0.0 \
    --port 8000 \
    --enable-lora \
    --max-lora-rank 64
```

ä½¿ç”¨APIè°ƒç”¨ï¼š
```bash
curl http://localhost:8000/v1/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "Qwen/Qwen2.5-0.5B-Instruct",
    "prompt": "è¯·ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±",
    "max_tokens": 500,
    "temperature": 0.8
  }'
```

## ğŸ§ª æ¨¡å‹æ¨ç†ç¤ºä¾‹

### Python APIæ¨ç†
```python
from vllm import LLM, SamplingParams

# é…ç½®é‡‡æ ·å‚æ•°
sampling_params = SamplingParams(
    temperature=0.8,
    top_p=0.9,
    max_tokens=200
)

# åŠ è½½æ¨¡å‹
llm = LLM(
    model="Qwen/Qwen2.5-0.5B-Instruct",
    enable_lora=True,
    max_lora_rank=64
)

# æ·»åŠ LoRAé€‚é…å™¨
llm.add_lora("./training/training/models/huanhuan_fast")

# ç”Ÿæˆæ–‡æœ¬
prompts = [
    "ä½ å¥½ï¼Œç”„å¬›ï¼Œä»Šå¤©å¤©æ°”å¦‚ä½•ï¼Ÿ",
    "è¯·é—®ä½ æ˜¯è°ï¼Ÿ"
]
outputs = llm.generate(prompts, sampling_params)

# æ‰“å°è¾“å‡º
for output in outputs:
    prompt = output.prompt
    generated_text = output.outputs[0].text
    print(f"Prompt: {prompt!r}, Generated text: {generated_text!r}")
```

### æ‰¹é‡æ¨ç†
```python
from vllm import LLM, SamplingParams

# é…ç½®é‡‡æ ·å‚æ•°
sampling_params = SamplingParams(
    temperature=0.8,
    top_p=0.9,
    max_tokens=300
)

# åŠ è½½æ¨¡å‹
llm = LLM(
    model="Qwen/Qwen2.5-0.5B-Instruct",
    enable_lora=True,
    max_lora_rank=64,
    tensor_parallel_size=1  # æ ¹æ®GPUæ•°é‡è°ƒæ•´
)

# æ·»åŠ LoRAé€‚é…å™¨
llm.add_lora("./training/training/models/huanhuan_fast")

# æ‰¹é‡ç”Ÿæˆ
prompts = [
    "çš‡ä¸Šï¼Œæ‚¨ä»Šæ—¥æ€ä¹ˆè¿™ä¹ˆé«˜å…´ï¼Ÿ",
    "åå¦ƒå¾…æˆ‘å¦‚ä½•ï¼Ÿ",
    "è¯·ä¸ºæˆ‘ä½œä¸€é¦–è¯—",
    "ä½ æœ€å–œæ¬¢ä»€ä¹ˆèŠ±ï¼Ÿ"
]

outputs = llm.generate(prompts, sampling_params)
for i, output in enumerate(outputs):
    print(f"é—®é¢˜ {i+1}: {prompts[i]}")
    print(f"å›ç­”: {output.outputs[0].text}\n")
```

## ğŸ› ï¸ æ€§èƒ½ä¼˜åŒ–å»ºè®®

### 1. GPUå†…å­˜ä¼˜åŒ–
```bash
# è®¾ç½®GPUå†…å­˜ä½¿ç”¨æ¯”ä¾‹
--gpu-memory-utilization 0.8

# è®¾ç½®æœ€å¤§æ¨¡å‹é•¿åº¦
--max-model-len 4096

# è®¾ç½®å¼ é‡å¹¶è¡Œå¤§å°ï¼ˆå¤šGPUï¼‰
--tensor-parallel-size 2
```

### 2. æ¨ç†å‚æ•°ä¼˜åŒ–
```python
sampling_params = SamplingParams(
    temperature=0.8,        # æ§åˆ¶è¾“å‡ºéšæœºæ€§
    top_p=0.9,              # æ ¸é‡‡æ ·
    top_k=40,               # top-ké‡‡æ ·
    repetition_penalty=1.05, # é‡å¤æƒ©ç½š
    max_tokens=512          # æœ€å¤§ç”Ÿæˆtokenæ•°
)
```

### 3. LoRAé…ç½®ä¼˜åŒ–
```bash
# è®¾ç½®æœ€å¤§LoRA rank
--max-lora-rank 64

# è®¾ç½®æœ€å¤§LoRAæ•°é‡
--max-loras 4
```

## ğŸ§¾ æ³¨æ„äº‹é¡¹

1. ç¡®ä¿åŸºç¡€æ¨¡å‹ Qwen/Qwen2.5-0.5B-Instruct å·²ä¸‹è½½æˆ–å¯è®¿é—®
2. LoRAé€‚é…å™¨è·¯å¾„å¿…é¡»æ­£ç¡®
3. GPUå†…å­˜è‡³å°‘éœ€è¦8GBä»¥ä¸Š
4. æ ¹æ®å®é™…ç¡¬ä»¶é…ç½®è°ƒæ•´å‚æ•°
5. å¦‚æœä½¿ç”¨Dockerï¼Œè¯·ç¡®ä¿å·²å®‰è£…NVIDIA Container Toolkit
6. å¦‚é‡åˆ°CUDAç›¸å…³é”™è¯¯ï¼Œå¯èƒ½éœ€è¦æŒ‡å®šæ­£ç¡®çš„CUDAç‰ˆæœ¬

## ğŸ“ APIè°ƒç”¨ç¤ºä¾‹

### å•æ¬¡å¯¹è¯
```bash
curl -X POST http://localhost:8000/v1/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "Qwen/Qwen2.5-0.5B-Instruct",
    "prompt": "ä½ å¥½ï¼Œç”„å¬›",
    "max_tokens": 300,
    "temperature": 0.8
  }'
```

### èŠå¤©æ¥å£ï¼ˆå¦‚æœä½¿ç”¨OpenAIå…¼å®¹APIï¼‰
```bash
curl -X POST http://localhost:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "Qwen/Qwen2.5-0.5B-Instruct",
    "messages": [
      {
        "role": "system",
        "content": "ä½ æ˜¯ç”„å¬›ï¼Œã€Šç”„å¬›ä¼ ã€‹ä¸­çš„å¥³ä¸»è§’..."
      },
      {
        "role": "user",
        "content": "çš‡ä¸Šï¼Œæ‚¨ä»Šæ—¥æ€ä¹ˆè¿™ä¹ˆé«˜å…´ï¼Ÿ"
      }
    ],
    "max_tokens": 300,
    "temperature": 0.8
  }'
```